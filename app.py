"""
Audio Transcription App â€” éŸ³é¢‘è‡ªåŠ¨åˆ‡åˆ†ä¸è½¬å†™
Flask backend: receives audio files, splits them with pydub,
transcribes each chunk via an OpenAI-compatible Gemini API, and
merges results.
"""

import os, uuid, base64, tempfile, threading, traceback
from pathlib import Path
from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from pydub import AudioSegment
from openai import OpenAI

app = Flask(__name__, static_folder="static", static_url_path="")
CORS(app)

UPLOAD_DIR = Path(tempfile.gettempdir()) / "audio_transcriber_uploads"
UPLOAD_DIR.mkdir(exist_ok=True)

# All audio formats supported by ffmpeg / pydub
SUPPORTED_EXTENSIONS = {
    ".wav", ".mp3", ".ogg", ".flac", ".aac", ".m4a",
    ".wma", ".aiff", ".aif", ".opus", ".amr", ".ape",
    ".ac3", ".webm", ".caf", ".spx", ".oga", ".wv",
    ".mp4", ".mov", ".mkv",  # video files with audio track
}

# In-memory task store  {task_id: {...}}
tasks: dict[str, dict] = {}

# â”€â”€ defaults (overridable per-request) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_MAX_CHUNK_MINUTES = 10       # minutes per chunk
DEFAULT_MAX_CHUNK_MB      = 20       # MB per chunk (safe for base64 inline)


# ================================================================
#  Utilities
# ================================================================

def split_audio(filepath: str, max_minutes: int, max_mb: int) -> list[str]:
    """
    Split an audio file by duration AND file-size constraints.
    Returns a list of temporary file paths for each chunk (always mp3).
    """
    audio = AudioSegment.from_file(filepath)
    fmt = "mp3"  # always export as mp3 for API compatibility

    chunk_ms = max_minutes * 60 * 1000
    chunks_by_time: list[AudioSegment] = []

    # First pass: split by time
    for start in range(0, len(audio), chunk_ms):
        chunks_by_time.append(audio[start:start + chunk_ms])

    # Second pass: further split any chunk that exceeds max_mb
    final_chunks: list[AudioSegment] = []
    for chunk in chunks_by_time:
        tmp = tempfile.NamedTemporaryFile(suffix=f".{fmt}", delete=False)
        chunk.export(tmp.name, format=fmt)
        tmp.close()
        size_mb = os.path.getsize(tmp.name) / (1024 * 1024)
        os.unlink(tmp.name)

        if size_mb <= max_mb:
            final_chunks.append(chunk)
        else:
            # Binary-split until every piece fits
            sub_chunks = _binary_split(chunk, fmt, max_mb)
            final_chunks.extend(sub_chunks)

    # Export final chunks
    paths: list[str] = []
    for i, chunk in enumerate(final_chunks):
        out = UPLOAD_DIR / f"{uuid.uuid4().hex}_{i}.{fmt}"
        chunk.export(str(out), format=fmt)
        paths.append(str(out))
    return paths


def _binary_split(segment: AudioSegment, fmt: str, max_mb: int) -> list[AudioSegment]:
    """Recursively halve a segment until each piece is â‰¤ max_mb."""
    tmp = tempfile.NamedTemporaryFile(suffix=f".{fmt}", delete=False)
    segment.export(tmp.name, format=fmt)
    tmp.close()
    size_mb = os.path.getsize(tmp.name) / (1024 * 1024)
    os.unlink(tmp.name)

    if size_mb <= max_mb:
        return [segment]
    mid = len(segment) // 2
    return _binary_split(segment[:mid], fmt, max_mb) + \
           _binary_split(segment[mid:], fmt, max_mb)


def transcribe_chunk(chunk_path: str, client: OpenAI, model: str) -> str:
    """Send a single audio chunk to the OpenAI-compatible API and return text."""
    mime = "audio/mpeg"  # chunks are always exported as mp3
    with open(chunk_path, "rb") as f:
        b64 = base64.b64encode(f.read()).decode()

    response = client.chat.completions.create(
        model=model,
        messages=[
            {
                "role": "system",
                "content": (
                    "ä½ æ˜¯ä¸€ä½æ‹¥æœ‰ 20 å¹´ç»éªŒçš„ä¸“ä¸šä¼šè®®é€Ÿè®°å‘˜å’Œçºªè¦ä¸“å®¶ã€‚"
                    "ä½ çš„ä»»åŠ¡æ˜¯å°†éŸ³é¢‘å†…å®¹è½¬åŒ–ä¸ºâ€œæ™ºèƒ½é€å­—ç¨¿â€ï¼ˆIntelligent Verbatimï¼‰ã€‚"
                    "æ ¸å¿ƒåŸåˆ™ï¼šä¿æŒå†…å®¹çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§ï¼ŒåŒæ—¶æå‡é˜…è¯»ä½“éªŒã€‚"
                ),
            },
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": (
                            "è¯·å¯¹ä»¥ä¸‹éŸ³é¢‘è¿›è¡Œä¸“ä¸šè½¬å†™ã€‚éµå¾ªä»¥ä¸‹ä¸šç•Œæ ‡å‡†è§„èŒƒï¼š\n\n"
                            "1. **æ™ºèƒ½å‡€åŒ–**ï¼š\n"
                            "   - å‰”é™¤å£è¯­åºŸè¯ï¼ˆå¦‚â€œé‚£ä¸ªâ€ã€â€œå‘ƒâ€ã€â€œå—¯â€ã€â€œå°±æ˜¯è¯´â€ç­‰ï¼‰ï¼Œé™¤éè¡¨è¾¾è¿Ÿç–‘æˆ–å¼ºè°ƒï¼›\n"
                            "   - ä¿®æ­£æ˜æ˜¾çš„å£è¯¯å’Œé‡å¤ï¼ˆå¦‚â€œæˆ‘...æˆ‘ä»¬è®¤ä¸ºâ€ -> â€œæˆ‘ä»¬è®¤ä¸ºâ€ï¼‰ï¼›\n"
                            "   - ä¿æŒåŸæœ¬çš„è¯­åºå’Œé€»è¾‘ï¼Œä¸è¦éšæ„æ”¹å†™å†…å®¹ã€‚\n\n"
                            "2. **æ ¼å¼è§„èŒƒ**ï¼š\n"
                            "   - **è¯´è¯äººæ ‡è®°**ï¼šæ ¹æ®å£°çº¹å’Œä¸Šä¸‹æ–‡åŒºåˆ†è¯´è¯äººï¼Œä½¿ç”¨ã€è¯´è¯äºº1ã€‘ã€ã€è¯´è¯äºº2ã€‘æˆ–å…·ä½“ç§°è°“ï¼ˆå¦‚ã€ä¸»æŒäººã€‘ã€ã€ç»ç†ã€‘ï¼‰æ ‡è®°ï¼›\n"
                            "   - **æ®µè½åˆ†æ˜**ï¼šä¸åŒè¯´è¯äººå¿…é¡»æ¢è¡Œã€‚é•¿æ®µç‹¬ç™½è¯·æ ¹æ®é€»è¾‘è¯­ä¹‰åˆç†åˆ†æ®µï¼›\n"
                            "   - **æ ‡ç‚¹ä¸“ä¸š**ï¼šä½¿ç”¨è§„èŒƒçš„ä¸­æ–‡å…¨è§’æ ‡ç‚¹ã€‚è¯­æ°”å¼ºçƒˆçš„ç”¨æ„Ÿå¹å·ï¼Œç–‘é—®ç”¨é—®å·ï¼Œå¹¶åˆ—ç”¨é¡¿å·ã€‚\n\n"
                            "3. **å…³é”®å†…å®¹çªå‡º**ï¼š\n"
                            "   - å¯¹äº**å…³é”®æ•°æ®**ï¼ˆé‡‘é¢ã€æ—¶é—´ã€æ•°é‡ï¼‰ã€**ä¸“æœ‰åè¯**ï¼ˆé¡¹ç›®åã€éƒ¨é—¨åã€æŠ€æœ¯æœ¯è¯­ï¼‰ï¼Œè¯·ç¡®ä¿å‡†ç¡®æ— è¯¯ï¼›\n"
                            "   - å¦‚é‡å†³ç­–æ€§ç»“è®ºæˆ–å¾…åŠäº‹é¡¹ï¼Œä¿æŒåŸè¯ï¼Œä¸è¦é—æ¼ã€‚\n\n"
                            "4. **æ··åˆè¯­è¨€å¤„ç†**ï¼š\n"
                            "   - ä¸­è‹±æ–‡æ··æ‚æ—¶ï¼Œè‹±æ–‡å•è¯å‰åä¿ç•™ç©ºæ ¼ï¼ˆå¦‚â€œä½¿ç”¨ AI æŠ€æœ¯â€ï¼‰ã€‚\n"
                            "   - ä»…è¾“å‡ºè½¬å†™æ­£æ–‡ï¼Œä¸è¦åŒ…å«â€œå¥½çš„â€ã€â€œä»¥ä¸‹æ˜¯è½¬å†™â€ç­‰æ— å…³å›å¤ã€‚"
                        ),
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:{mime};base64,{b64}",
                        },
                    },
                ],
            }
        ],
        max_tokens=8192,
    )
    return response.choices[0].message.content.strip()


def run_transcription(task_id: str, filepath: str,
                      base_url: str, api_key: str, model: str,
                      max_minutes: int, max_mb: int):
    """Background worker: split â†’ transcribe â†’ merge."""
    task = tasks[task_id]
    try:
        # â”€â”€ 1. Split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        task["status"] = "splitting"
        chunks = split_audio(filepath, max_minutes, max_mb)
        task["total_chunks"] = len(chunks)
        task["status"] = "transcribing"

        # â”€â”€ 2. Transcribe each chunk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        client = OpenAI(base_url=base_url, api_key=api_key)
        results: list[str] = []
        for i, chunk_path in enumerate(chunks):
            task["current_chunk"] = i + 1          # which chunk is being processed NOW
            task["completed_chunks"] = i            # how many are fully done
            try:
                text = transcribe_chunk(chunk_path, client, model)
                results.append(text)
                task["chunk_results"].append({
                    "index": i + 1,
                    "status": "done",
                    "text": text,
                })
            except Exception as e:
                err_msg = str(e)
                results.append(f"[ç‰‡æ®µ {i+1} è½¬å†™å¤±è´¥: {err_msg}]")
                task["chunk_results"].append({
                    "index": i + 1,
                    "status": "error",
                    "text": err_msg,
                })
            finally:
                task["completed_chunks"] = i + 1
                # Clean up chunk file
                try:
                    os.unlink(chunk_path)
                except OSError:
                    pass

        # â”€â”€ 3. Merge â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        task["transcript"] = "\n\n".join(results)
        task["status"] = "done"

    except Exception:
        task["status"] = "error"
        task["error"] = traceback.format_exc()
    finally:
        # Clean up uploaded file
        try:
            os.unlink(filepath)
        except OSError:
            pass


# ================================================================
#  Routes
# ================================================================

@app.route("/")
def index():
    return send_from_directory("static", "index.html")


@app.route("/api/upload", methods=["POST"])
def upload():
    file = request.files.get("audio")
    if not file:
        return jsonify({"error": "æœªæ”¶åˆ°éŸ³é¢‘æ–‡ä»¶"}), 400

    ext = Path(file.filename).suffix.lower()
    if ext not in SUPPORTED_EXTENSIONS:
        supported = ", ".join(sorted(SUPPORTED_EXTENSIONS))
        return jsonify({"error": f"ä¸æ”¯æŒçš„æ ¼å¼ {ext}ï¼Œæ”¯æŒ: {supported}"}), 400

    base_url  = request.form.get("base_url", "").strip()
    api_key   = request.form.get("api_key", "").strip()
    model     = request.form.get("model", "").strip()

    if not all([base_url, api_key, model]):
        return jsonify({"error": "è¯·å¡«å†™ Base URLã€API Key å’Œ Model"}), 400

    max_minutes = int(request.form.get("max_minutes", DEFAULT_MAX_CHUNK_MINUTES))
    max_mb      = int(request.form.get("max_mb", DEFAULT_MAX_CHUNK_MB))

    # Save uploaded file
    task_id = uuid.uuid4().hex[:12]
    save_path = str(UPLOAD_DIR / f"{task_id}{ext}")
    file.save(save_path)

    file_size_mb = os.path.getsize(save_path) / (1024 * 1024)

    tasks[task_id] = {
        "status": "queued",
        "filename": file.filename,
        "file_size_mb": round(file_size_mb, 2),
        "total_chunks": 0,
        "current_chunk": 0,
        "completed_chunks": 0,
        "chunk_results": [],
        "transcript": "",
        "error": "",
    }

    t = threading.Thread(
        target=run_transcription,
        args=(task_id, save_path, base_url, api_key, model, max_minutes, max_mb),
        daemon=True,
    )
    t.start()

    return jsonify({"task_id": task_id, "file_size_mb": round(file_size_mb, 2)})


@app.route("/api/status/<task_id>")
def status(task_id):
    task = tasks.get(task_id)
    if not task:
        return jsonify({"error": "ä»»åŠ¡ä¸å­˜åœ¨"}), 404
    return jsonify(task)


@app.route("/api/test-connection", methods=["POST"])
def test_connection():
    data = request.json or {}
    base_url = data.get("base_url", "").strip()
    api_key  = data.get("api_key", "").strip()
    model    = data.get("model", "").strip()

    if not all([base_url, api_key, model]):
        return jsonify({"ok": False, "error": "è¯·å¡«å†™æ‰€æœ‰é…ç½®é¡¹"}), 400

    try:
        client = OpenAI(base_url=base_url, api_key=api_key)
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": "Hi, reply with OK"}],
            max_tokens=10,
        )
        return jsonify({"ok": True, "reply": resp.choices[0].message.content})
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)})


# ================================================================

if __name__ == "__main__":
    print("ğŸ™ï¸  Audio Transcriber running on http://localhost:5099")
    app.run(host="0.0.0.0", port=5099, debug=False)
